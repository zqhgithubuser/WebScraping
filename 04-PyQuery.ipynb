{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyquery.pyquery.PyQuery"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyquery import PyQuery as pq\n",
    "import requests\n",
    "\n",
    "response = requests.get(\"https://webscraper.io/test-sites/\")\n",
    "source = pq(response.content)\n",
    "type(source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 元素遍历、属性和伪类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 查找元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<title>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source.find('title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Test Sites | Web Scraper'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source.find('title').text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获取属性值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You need to train your web scraper? We have created simple test sites that allow you to try all corner cases and proof test your scraper. Try it now.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source.find(\"meta[name='description']\").attr('content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'web scraping,Web Scraper,Chrome extension,Crawling,Cross platform scraper'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source.find(\"meta[name='keywords']\").attr('content')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 伪类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Toggle navigation'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source.find('a:eq(0)').text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Web Scraper'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source.find('a.menuitm:first').text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pricing'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source.find('a.menuitm:last').text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/cloud-scraper'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source.find('a.menuitm:eq(1)').attr('href')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<button.navbar-toggler.float-end.collapsed>, <button#dropdownMenuLink.menuitm.nav-link.dropdown-toggle>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source.find(':input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<h1>, <h2.site-heading>, <h2.site-heading>, <h2.site-heading>, <h2.site-heading>, <h2.site-heading>, <h2.site-heading>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source.find(':header')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<meta>, <meta>, <meta>, <meta>, <link>, <meta>, <link>, <link>, <link>, <link>, <link>, <link>, <link>, <link>, <script>, <iframe>, <span.icon-bar.top-bar>, <span.icon-bar.middle-bar>, <span.icon-bar.bottom-bar>, <span.icon-bar.extra-bottom-bar>, <img>, <div.crta>, <div.crta>, <div.crta>, <span.crta>, <hr.test-site-divider>, <img>, <hr.test-site-divider>, <img>, <hr.test-site-divider>, <img>, <hr.test-site-divider>, <img>, <hr.test-site-divider>, <img>, <hr.test-site-divider>, <img>, <div.clearfix>, <div.push>, <br>, <i.ws-icon.ws-icon-facebook-f>, <i.ws-icon.ws-icon-twitter>, <i.ws-icon.ws-icon-linkedin>, <i.ws-icon.ws-icon-youtube>, <i.ws-icon.ws-icon-chrome-dark>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source.find(':empty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<meta>, <meta>, <meta>, <meta>, <meta>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source.find('meta:empty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<meta>, <meta>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source.find('meta:empty:odd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<meta>, <meta>, <meta>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source.find('meta:empty:even')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<h2.site-heading>, <h2.site-heading>, <h2.site-heading>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source.find(':header:odd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a.nav-link.menuitm>, <a>, <a>, <a>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source.find(\"a:contains('Web')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a>]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source.find('a:contains(\"Web\")').eq(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Website Privacy Policy'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source.find(\"a:contains('Web')\").eq(-1).text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 迭代"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/', '/cloud-scraper', '/pricing']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[item.attr('href') for item in source.find('a.menuitm').items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/',\n",
       " '/cloud-scraper',\n",
       " '/pricing',\n",
       " 'https://chromewebstore.google.com/detail/web-scraper-free-web-scra/jnhgnonknehpejjnehehllkliplmbmhn?hl=en',\n",
       " 'https://cloud.webscraper.io/']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[item.attr('href') for item in source.find(\"a.menuitm, a[class*='btn-menu']\").items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用PyQuery进行网络爬虫"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 示例一：书籍详细信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导入库、定义 URLs、默认分页值和一个空数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyquery import PyQuery as pq\n",
    "import requests\n",
    "import math\n",
    "\n",
    "category = 'Childrens'\n",
    "siteUrl = 'http://books.toscrape.com/'\n",
    "baseUrl = siteUrl + 'catalogue/category/books/childrens_11/index.html'\n",
    "pageUrl = siteUrl + 'catalogue/category/books/childrens_11/page-'\n",
    "dataSet = []\n",
    "page = 1\n",
    "totalPagesCount = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "循环处理每一页"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1 from Total 2...\n",
      "Page 2 from Total 2...\n"
     ]
    }
   ],
   "source": [
    "while page <= totalPagesCount:\n",
    "    response = requests.get(pageUrl + str(page) + '.html')\n",
    "    source = pq(response.content)\n",
    "\n",
    "    if page == 1:\n",
    "        pageValues =  [value.text() for value in source.find('form.form-horizontal strong').items()]\n",
    "        if len(pageValues) > 0:\n",
    "            pageValues = list(map(int, pageValues))\n",
    "            totalPagesCount = math.ceil(pageValues[0] / pageValues[2])\n",
    "    print(f'Page {page} from Total {totalPagesCount}...')     \n",
    "\n",
    "    books = source.find('article.product_pod')\n",
    "    for book in books.items():\n",
    "        image = book.find('.image_container a img').attr('src')\n",
    "        rating = book.find('p.star-rating').attr('class').split()[1]\n",
    "        title = book.find('h3 a').attr('title')\n",
    "        url = book.find('h3 a').attr('href')\n",
    "        price = book.find('p.price_color').text()\n",
    "        stock = book.find('p.availability').attr('class').split()[0]\n",
    "\n",
    "        dataSet.append({\n",
    "            'name': title,\n",
    "            'price': price,\n",
    "            'stock': stock,\n",
    "            'rating': rating,\n",
    "            'image': image.replace(\n",
    "                '../../../../', 'http://books.toscrape.com/catalogue'\n",
    "            ),\n",
    "            'url': url.replace(\n",
    "                '../../../', 'http://books.toscrape.com/catalogue/'\n",
    "            )\n",
    "        })\n",
    "\n",
    "    page += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将收集的数据写入 JSON 文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('downloads/childrens_books.json', 'w') as fp:\n",
    "    json.dump(dataSet, fp, indent=2, sort_keys=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 示例二：站点地图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导入库、定义 URL 以及 CSV 文件的字段名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Children Length: 530\n"
     ]
    }
   ],
   "source": [
    "from pyquery import PyQuery as pq\n",
    "import requests\n",
    "import csv\n",
    "\n",
    "url = \"https://www.schools.com/sitemap.xml\"\n",
    "columns=['loc', 'lastmod', 'changefreq', 'priority'] \n",
    "xmlFile = requests.get(url).content\n",
    "urlXML = pq(xmlFile, parser='html')\n",
    "print(f'Children Length: {len(urlXML.children())}')    # Child-Length: 530"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "循环处理每个 `<url>` 节点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSet = []\n",
    "\n",
    "for loop in range(len(urlXML.children())):\n",
    "    child = urlXML.children().eq(loop)\n",
    "    dataSet.append({\n",
    "        child.find('loc').text(),\n",
    "        child.find('lastmod').text(),\n",
    "        child.find('changefreq').text(),\n",
    "        child.find('priority').text()\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将收集的数据写入 CSV 文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_csv(data, filename, columns):\n",
    "    with open(filename, 'w+', newline='', encoding=\"utf-8\") as fp:\n",
    "        writer = csv.DictWriter(fp, fieldnames=columns)\n",
    "        writer.writeheader()\n",
    "        writer = csv.writer(fp)\n",
    "        for element in dataSet:\n",
    "            writer.writerow(element)\n",
    "\n",
    "write_to_csv(dataSet, 'downloads/schoolXML.csv', columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 示例三：带有作者详细信息的名言"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "变量定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyquery import PyQuery as pq\n",
    "import requests\n",
    "import csv\n",
    "\n",
    "url = 'http://quotes.toscrape.com/tag/books/page/'\n",
    "columns = ['id', 'author', 'quote', 'tags', 'quote_length', 'born_date', 'born_location', 'author_url'] \n",
    "authorSet = {}\n",
    "dataSet = []\n",
    "page = 1\n",
    "nextPage = True\n",
    "uid = 0  # 编号"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "循环处理每一页"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://quotes.toscrape.com/tag/books/page/1\n",
      "Quotes to Scrape\n",
      "Processing 1...\n",
      "Author (Mark_Twain) details already found!\n",
      "Author (Jane_Austen) details already found!\n",
      "http://quotes.toscrape.com/tag/books/page/2\n",
      "Quotes to Scrape\n",
      "Processing 2...\n"
     ]
    }
   ],
   "source": [
    "while nextPage:\n",
    "    print(url + str(page))\n",
    "    response = requests.get(url + str(page))\n",
    "    source = pq(response.content)\n",
    "    print(source.find('title').text())\n",
    "\n",
    "    if source.find(\"ul.pager li.next:contains('Next')\"):    # 当前是否是最后一页\n",
    "        nextPage = True\n",
    "    else:\n",
    "        nextPage = False\n",
    "    \n",
    "    print(f\"Processing {page}...\") \n",
    "    for quote in source.find('.quote').items():\n",
    "        quoteText = quote.find(\"[itemprop='text']\").text()\n",
    "        author = quote.find(\"[itemprop='author']\").text()\n",
    "        tags = quote.find(\"[itemprop='keywords']\").attr('content').replace(',', '|')\n",
    "        authorUrl = quote.find(\"a[href*='/author/']\").attr('href')\n",
    "\n",
    "        if authorUrl:\n",
    "            # print(authorUrl)\n",
    "            authorKey = author.replace('.', '_').replace(' ', '_')\n",
    "        \n",
    "        if authorUrl and authorKey not in authorSet.keys():\n",
    "            authorUrl = 'http://quotes.toscrape.com' + authorUrl  # 完整URL\n",
    "            source_author = pq(requests.get(authorUrl).content)\n",
    "            bornDate = source_author.find('.author-born-date').text()\n",
    "            bornLocation = source_author.find('.author-born-location').text().replace('in', '').strip()\n",
    "            authorSet[authorKey] = {\n",
    "                'name': author,\n",
    "                'url': authorUrl,\n",
    "                'date': bornDate,\n",
    "                'location': bornLocation\n",
    "            }\n",
    "        else:\n",
    "            print(f'Author ({authorKey}) details already found!')\n",
    "\n",
    "        uid += 1\n",
    "\n",
    "        dataSet.append([\n",
    "            uid, author, quoteText, tags, len(quoteText),\n",
    "            authorSet[authorKey]['date'],\n",
    "            authorSet[authorKey]['location'],\n",
    "            authorSet[authorKey]['url']\n",
    "        ])\n",
    "        \n",
    "    page += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Jane_Austen': {'name': 'Jane Austen',\n",
       "  'url': 'http://quotes.toscrape.com/author/Jane-Austen',\n",
       "  'date': 'December 16, 1775',\n",
       "  'location': 'Steventon Rectory, Hampshire, The United Kgdom'},\n",
       " 'Mark_Twain': {'name': 'Mark Twain',\n",
       "  'url': 'http://quotes.toscrape.com/author/Mark-Twain',\n",
       "  'date': 'November 30, 1835',\n",
       "  'location': 'Florida, Missouri, The United States'},\n",
       " 'Jorge_Luis_Borges': {'name': 'Jorge Luis Borges',\n",
       "  'url': 'http://quotes.toscrape.com/author/Jorge-Luis-Borges',\n",
       "  'date': 'August 24, 1899',\n",
       "  'location': 'Buenos Aires, Argenta'},\n",
       " 'C_S__Lewis': {'name': 'C.S. Lewis',\n",
       "  'url': 'http://quotes.toscrape.com/author/C-S-Lewis',\n",
       "  'date': 'November 29, 1898',\n",
       "  'location': 'Belfast, Ireland'},\n",
       " 'Haruki_Murakami': {'name': 'Haruki Murakami',\n",
       "  'url': 'http://quotes.toscrape.com/author/Haruki-Murakami',\n",
       "  'date': 'January 12, 1949',\n",
       "  'location': 'Kyoto, Japan'},\n",
       " 'Ernest_Hemingway': {'name': 'Ernest Hemingway',\n",
       "  'url': 'http://quotes.toscrape.com/author/Ernest-Hemingway',\n",
       "  'date': 'July 21, 1899',\n",
       "  'location': 'Oak Park, Illois, The United States'},\n",
       " 'J_D__Salinger': {'name': 'J.D. Salinger',\n",
       "  'url': 'http://quotes.toscrape.com/author/J-D-Salinger',\n",
       "  'date': 'January 01, 1919',\n",
       "  'location': 'Manhattan, New York, The United States'},\n",
       " \"Madeleine_L'Engle\": {'name': \"Madeleine L'Engle\",\n",
       "  'url': 'http://quotes.toscrape.com/author/Madeleine-LEngle',\n",
       "  'date': 'November 29, 1918',\n",
       "  'location': 'New York City, New York, The United States'},\n",
       " 'George_R_R__Martin': {'name': 'George R.R. Martin',\n",
       "  'url': 'http://quotes.toscrape.com/author/George-R-R-Martin',\n",
       "  'date': 'September 20, 1948',\n",
       "  'location': 'Bayonne, New Jersey, The United States'}}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authorSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将收集的数据写入 JSON 文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('downloads/quotes_author.json', 'w') as fp:\n",
    "    json.dump(authorSet, fp, indent=2, sort_keys=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
